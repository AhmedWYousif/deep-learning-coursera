
# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

## About this Course

This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. 

After finish, you will: 
- Understand industry **best-practices** for building deep learning applications. 
- Be able to effectively use the common neural network **"tricks"**, including initialization, **L2** and **dropout regularization**, **Batch normalization**, **gradient checking**, 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow. 

## Syllabus 
- [Practical aspects of Deep Learning](Week%201%20-%20Practical%20aspects%20of%20Deep%20Learning)
  * [Initialization](Week%201%20-%20Practical%20aspects%20of%20Deep%20Learning/Initialization/Initialization.ipynb)
  * [Regularization](Week%201%20-%20Practical%20aspects%20of%20Deep%20Learning/Regularization/Regularization.ipynb)
  * [Gradient Checking](Week%201%20-%20Practical%20aspects%20of%20Deep%20Learning/Gradient%20Checking/Gradient%2BChecking.ipynb)
- Optimization algorithms
- Hyperparameter tuning, Batch Normalization and Programming Frameworks
