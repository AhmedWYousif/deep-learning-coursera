
# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

## About this Course

This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. 

After finish, you will: 
- Understand industry **best-practices** for building deep learning applications. 
- Be able to effectively use the common neural network **"tricks"**, including initialization, **L2** and **dropout regularization**, **Batch normalization**, **gradient checking**, 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow. 

## Syllabus 
- [Practical aspects of Deep Learning](W1%20-%20Practical%20aspects%20of%20Deep%20Learning)
  * [Initialization Programming Assignment](W1%20-%20Practical%20aspects%20of%20Deep%20Learning/Initialization/Initialization.ipynb)
  * [Regularization Programming Assignment](W1%20-%20Practical%20aspects%20of%20Deep%20Learning/Regularization/Regularization.ipynb)
  * [Gradient Checking Programming Assignment](W1%20-%20Practical%20aspects%20of%20Deep%20Learning/Gradient%20Checking/Gradient%2BChecking.ipynb)
- [Optimization algorithms](W2%20-%20Optimization%20algorithms)
  * [Optimization Programming Assignment](W2%20-%20Optimization%20algorithms/Optimization_methods.ipynb)
- Hyperparameter tuning, Batch Normalization and Programming Frameworks
